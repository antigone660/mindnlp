{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(12002:140052698522816,MainProcess):2024-05-14-19:50:24.413.323 [mindspore/run_check/_check_version.py:102] MindSpore version 2.2.13 and cuda version 11.8.89 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
      "/hpc2hdd/home/ypeng455/.conda/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.702 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [01:06<00:00,  8.26s/it]\n",
      "The following parameters in checkpoint files are not loaded:\n",
      "['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq']\n",
      "/hpc2hdd/home/ypeng455/mindnlpV3/mindnlp/mindnlp/transformers/generation/utils.py:1402: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://hf-mirror.com/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "[WARNING] KERNEL(12002,7f5e1f7fe700,python):2024-05-14-19:51:46.841.106 [mindspore/ccsrc/kernel/kernel.h:376] CheckShapeNull] For 'Equal', the shape of input_0 cannot contain zero, but got [const vector]{1, 0}\n",
      "[WARNING] KERNEL(12002,7f608f5864c0,python):2024-05-14-19:51:46.847.655 [mindspore/ccsrc/kernel/kernel.h:376] CheckShapeNull] For 'BitwiseAnd', the shape of input_0 cannot contain zero, but got [const vector]{1, 0}\n",
      "[WARNING] KERNEL(12002,7f608f5864c0,python):2024-05-14-19:51:46.848.234 [mindspore/ccsrc/kernel/kernel.h:376] CheckShapeNull] For 'Cast', the shape of input cannot contain zero, but got [const vector]{1, 0}\n",
      "[WARNING] KERNEL(12002,7f5f09687700,python):2024-05-14-19:51:46.997.504 [mindspore/ccsrc/kernel/kernel.h:376] CheckShapeNull] For 'Mul', the shape of input_0 cannot contain zero, but got [const vector]{0, 11008}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a flowchart that illustrates the process of how a visual language model and a visual expert built on a language model work. On the left, the visual language model takes an image and a textual description, processes them through an MLP adapter, and then through a VT encoder and a word embedding layer to produce a paired text. On the right, the visual expert model takes an image and a textual description, processes them through a multi-head attention mechanism with a QKV matrix, and then through a layer norm and text features to produce a text.</s>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "以下是推理结果，有几个warning，但推理结果和torch一致\n",
    "\"\"\"\n",
    "from mindnlp.transformers import CogVLMConfig,MLP,RMSNorm,PretrainedConfig,LlamaTokenizer,AutoModelForCausalLM\n",
    "import mindspore\n",
    "mindspore.set_seed(42)\n",
    "ms_net = AutoModelForCausalLM.from_pretrained(\n",
    "    'THUDM/cogvlm-chat-hf'\n",
    ")\n",
    "from PIL import Image\n",
    "tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n",
    "query = 'describe this picture'\n",
    "image = Image.open('CogVLM.png').convert('RGB')\n",
    "ms_inputs = ms_net.build_conversation_input_ids(tokenizer, query=query, history=[],images=[image])  # chat mode\n",
    "\n",
    "ms_inputs = {\n",
    "    'input_ids': ms_inputs['input_ids'].unsqueeze(0),\n",
    "    'token_type_ids': ms_inputs['token_type_ids'].unsqueeze(0),\n",
    "    'attention_mask': ms_inputs['attention_mask'].unsqueeze(0),\n",
    "    'images': [[mindspore.Tensor(ms_inputs['images'][0],dtype=mindspore.float16)]],\n",
    "}\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "ms_out = ms_net.generate(**ms_inputs,**gen_kwargs)\n",
    "outputs = ms_out[:, ms_inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好! 很感谢您咨询. 请问您具备哪些��</s>\n"
     ]
    }
   ],
   "source": [
    "query = '你好'\n",
    "text_only_template = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {query} ASSISTANT:\" \n",
    "query = text_only_template\n",
    "image = Image.open('/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/CogVLM.png').convert('RGB')\n",
    "ms_inputs = ms_net.build_conversation_input_ids(tokenizer, query=query, history=[]) # chat mode\n",
    "\n",
    "ms_inputs = {\n",
    "    'input_ids': ms_inputs['input_ids'].unsqueeze(0),\n",
    "    'token_type_ids': ms_inputs['token_type_ids'].unsqueeze(0),\n",
    "    'attention_mask': ms_inputs['attention_mask'].unsqueeze(0),\n",
    "}\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "ms_out = ms_net.generate(**ms_inputs,**gen_kwargs)\n",
    "outputs = ms_out[:, ms_inputs['input_ids'].shape[1]:]\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm', '/hpc2hdd/home/ypeng455/.conda/envs/py39/lib/python39.zip', '/hpc2hdd/home/ypeng455/.conda/envs/py39/lib/python3.9', '/hpc2hdd/home/ypeng455/.conda/envs/py39/lib/python3.9/lib-dynload', '', '/hpc2hdd/home/ypeng455/.conda/envs/py39/lib/python3.9/site-packages', '/hpc2hdd/home/ypeng455/mindnlp', '/tmp/tmpfb5wzsck']\n",
      "MindSpore version:  2.2.13\n",
      "The result of multiplication calculation is correct, MindSpore has been installed on platform [GPU] successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "这之后的内容是精度校验的记录,最差模块的精度在1e-4以内。\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "from argparse import Namespace\n",
    "from typing import TYPE_CHECKING, Optional, Tuple, List, Union, Literal, Dict, Any\n",
    "import mindspore\n",
    "import torch\n",
    "import numpy as np\n",
    "import modeling_cogvlm_pt\n",
    "import visual_pt\n",
    "from mindspore import ops\n",
    "from einops import rearrange\n",
    "from configuration_cogvlm_pt import CogVLMConfig as CogVLMConfigpt\n",
    "from mindnlp.transformers import CogVLMConfig,MLP,RMSNorm,PretrainedConfig,LlamaTokenizer,AutoModelForCausalLM\n",
    "from mindspore.amp import auto_mixed_precision\n",
    "from PIL import Image\n",
    "config_pt = CogVLMConfigpt()\n",
    "config_pt = config_pt.from_dict(PretrainedConfig.get_config_dict(r'/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/THUDM/cogvlm-chat-hf')[0])\n",
    "mindspore.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "config_ms = CogVLMConfig()\n",
    "device = torch.device('cuda')\n",
    "config_ms = config_ms.from_dict(PretrainedConfig.get_config_dict(r'/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/THUDM/cogvlm-chat-hf')[0])\n",
    "vision_config = Namespace(**config_ms.vision_config)\n",
    "LANGUAGE_TOKEN_TYPE = 0\n",
    "VISION_TOKEN_TYPE = 1\n",
    "mindspore.set_context(device_target='GPU')\n",
    "mindspore.run_check()\n",
    "\n",
    "# CogVLMForCausalLM\n",
    "from mindnlp.transformers import CogVLMForCausalLM\n",
    "ms_net = CogVLMForCausalLM(config_ms)\n",
    "pt_net =  modeling_cogvlm_pt.CogVLMForCausalLM(config_pt).to('cuda')\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "\n",
    "pt_keys = list(pt_dict.keys())\n",
    "for i in pt_keys:\n",
    "    if 'inv_freq' in i:\n",
    "        pt_keys.remove(i)\n",
    "ms_keys = list(ms_net.parameters_dict().keys())\n",
    "assert pt_keys == ms_keys\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).cpu().detach().numpy()))\n",
    "\n",
    "from PIL import Image\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/lmsys/vicuna-7b-v1.5')\n",
    "query = '你好！你可以用中文描述一下这张图片里有什么呢'\n",
    "image = Image.open('/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/CogVLM.png').convert('RGB')\n",
    "ms_inputs = ms_net.build_conversation_input_ids(tokenizer, query=query, history=[],images=[image])  # chat mode\n",
    "\n",
    "ms_inputs = {\n",
    "    'input_ids': ms_inputs['input_ids'].unsqueeze(0),\n",
    "    'token_type_ids': ms_inputs['token_type_ids'].unsqueeze(0),\n",
    "    'attention_mask': ms_inputs['attention_mask'].unsqueeze(0),\n",
    "    'images': [[mindspore.Tensor(ms_inputs['images'][0])]],\n",
    "}\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "#ms_out = ms_net.generate(**ms_inputs,**gen_kwargs)\n",
    "ms_out = ms_net(input_ids=ms_inputs['input_ids'],images=ms_inputs['images'],attention_mask=ms_inputs['attention_mask'],token_type_ids=ms_inputs['token_type_ids'])\n",
    "\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer,PretrainedConfig\n",
    "tokenizer = LlamaTokenizer.from_pretrained('/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/lmsys/vicuna-7b-v1.5')\n",
    "\n",
    "query = '你好！你可以用中文描述一下这张图片里有什么呢'\n",
    "image = Image.open('/hpc2hdd/home/ypeng455/mindnlp/mindnlp/transformers/models/cogvlm/CogVLM.png').convert('RGB')\n",
    "inputs = pt_net.build_conversation_input_ids(tokenizer, query=query, history=[],images=[image])  # chat mode\n",
    "inputs = {\n",
    "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
    "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
    "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
    "    'images': [[inputs['images'][0].to('cuda').to(torch.float32)]],\n",
    "}\n",
    "\n",
    "pt_out = pt_net(**inputs)\n",
    "\n",
    "\n",
    "assert pt_out[0].shape == ms_out[0].shape\n",
    "loss = 1e-4                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out[0].cpu().detach().numpy(), ms_out[0].asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP\n",
    "from mindnlp.transformers import EVA2CLIPModel\n",
    "ms_clip = EVA2CLIPModel(config_ms)\n",
    "# ms_clip.to_float(mindspore.float16)\n",
    "pt_clip =  modeling_cogvlm_pt.EVA2CLIPModel(config_pt).to('cuda')\n",
    "pytorch_state_dict = pt_clip.state_dict()\n",
    "\n",
    "pt_dict = pt_clip.state_dict()\n",
    "\n",
    "pt_keys = list(pt_dict.keys())\n",
    "for i in pt_keys:\n",
    "    if 'inv_freq' in i:\n",
    "        pt_keys.remove(i)\n",
    "ms_keys = list(ms_clip.parameters_dict().keys())\n",
    "assert pt_keys == ms_keys\n",
    "for key, parameter in ms_clip.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_np = np.random.rand(2,3,490,490)\n",
    "\n",
    "input_id_ms = mindspore.Tensor(input_id_np,dtype=mindspore.float32)\n",
    "input_id_pt = torch.Tensor(input_id_np).to(device).to(torch.float32)\n",
    "\n",
    "ms_out = ms_clip(input_id_ms)\n",
    "pt_out = pt_clip(input_id_pt)\n",
    "\n",
    "assert pt_out[0].shape == ms_out[0].shape\n",
    "loss = 1e-5                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out.cpu().detach().numpy(), ms_out.asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_transformer_attention_input\n",
      "vision_transformer_attention_inner_out\n",
      "vision_transformer_attention_inner_dense\n",
      "vision_transformer_attention_output\n",
      "vision_transformer_attention_layernorm\n",
      "vision_transformer_hidden_states\n",
      "vision_transformer_mlp_output\n",
      "images\n",
      "patch\n",
      "transformer\n",
      "slice\n",
      "glu_linear\n",
      "glu_norm\n",
      "glu_act1\n",
      "glu_act2\n",
      "glu_act3\n",
      "linear\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "file_name_list = ['vision_transformer_attention_input','vision_transformer_attention_inner_out','vision_transformer_attention_inner_dense',\n",
    "'vision_transformer_attention_output','vision_transformer_attention_layernorm','vision_transformer_hidden_states','vision_transformer_mlp_output',\n",
    "'images','patch','transformer','slice','glu_linear','glu_norm','glu_act1','glu_act2','glu_act3','linear','cat']\n",
    "ms_prefix = '_ms.npy'\n",
    "pt_prefix = '_pt.npy'\n",
    "pt_pp=''\n",
    "pt_list = []\n",
    "ms_list = []\n",
    "for i,file in enumerate(file_name_list):\n",
    "    \n",
    "    ms_list.append(np.load(file+ms_prefix))\n",
    "    pt_list.append(np.load(pt_pp+file+pt_prefix))\n",
    "\n",
    "\n",
    "loss = 1e-4                                  # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "for i in range(len(ms_list)):\n",
    "    print(file_name_list[i])\n",
    "    assert np.allclose(ms_list[i],pt_list[i] , loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['self_attn.vision_expert_query_key_value.weight', 'self_attn.vision_expert_dense.weight', 'self_attn.language_expert_query_key_value.weight', 'self_attn.language_expert_dense.weight', 'mlp.language_mlp.gate_proj.weight', 'mlp.language_mlp.up_proj.weight', 'mlp.language_mlp.down_proj.weight', 'mlp.vision_mlp.gate_proj.weight', 'mlp.vision_mlp.up_proj.weight', 'mlp.vision_mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight']\n",
      "['self_attn.vision_expert_query_key_value.weight', 'self_attn.vision_expert_dense.weight', 'self_attn.language_expert_query_key_value.weight', 'self_attn.language_expert_dense.weight', 'mlp.language_mlp.gate_proj.weight', 'mlp.language_mlp.up_proj.weight', 'mlp.language_mlp.down_proj.weight', 'mlp.vision_mlp.gate_proj.weight', 'mlp.vision_mlp.up_proj.weight', 'mlp.vision_mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight']\n"
     ]
    }
   ],
   "source": [
    "# cogvlmDecoderLayer\n",
    "from mindnlp.transformers import CogVLMDecoderLayer\n",
    "ms_net = CogVLMDecoderLayer(config_ms)\n",
    "pt_net = modeling_cogvlm_pt.CogVLMDecoderLayer(config_ms)\n",
    "pt_net.to(device)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "\n",
    "pt_keys = list(pt_dict.keys())\n",
    "pt_keys.remove('self_attn.rotary_emb.inv_freq')\n",
    "ms_keys = list(ms_net.parameters_dict().keys())\n",
    "print(pt_keys)\n",
    "print(ms_keys)\n",
    "assert pt_keys == ms_keys\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).cpu().detach().numpy()))\n",
    "\n",
    "\n",
    "file_name_list = ['hidden_states','token_type_ids','position_ids','attention_mask']\n",
    "ms_prefix = '_ms.npy'\n",
    "pt_prefix = '_pt.npy'\n",
    "np_list = []\n",
    "for i,file in enumerate(file_name_list):\n",
    "    assert np.all(np.load(file+ms_prefix) == np.load(file+pt_prefix))\n",
    "    np_list.append(np.load(file+ms_prefix))\n",
    "ms_list = [mindspore.Tensor(i) for i in np_list]\n",
    "pt_list = [torch.Tensor(i).to(device) for i in np_list]\n",
    "\n",
    "ms_out = ms_net(ms_list[0],ms_list[1],ms_list[2],ms_list[3])\n",
    "pt_out = pt_net(pt_list[0],pt_list[1].long(),pt_list[2].long(),pt_list[-1])\n",
    "\n",
    "assert pt_out[0].shape == ms_out[0].shape\n",
    "loss = 1e-5                                  # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out[0].cpu().detach().numpy(), ms_out[0].asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP test\n",
    "ms_net = MLP(config_ms)\n",
    "pt_net = modeling_cogvlm_pt.MLP(config_ms)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "assert pt_dict.keys() == ms_net.parameters_dict().keys()\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_np = np.random.random((2,config_ms.hidden_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "assert pt_net(input_pt).shape == ms_net(input_ms).shape\n",
    "loss = 1e-5                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_net(input_pt).detach().numpy(), ms_net(input_ms).asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSNorm test\n",
    "ms_net = RMSNorm(config_ms.hidden_size,eps=config_ms.rms_norm_eps)\n",
    "pt_net = modeling_cogvlm_pt.RMSNorm(config_ms.hidden_size,eps=config_ms.rms_norm_eps)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "assert pt_dict.keys() == ms_net.parameters_dict().keys()\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_np = np.random.random((2,3,config_ms.hidden_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "pt_out = pt_net(input_pt)\n",
    "ms_out = ms_net(input_ms)\n",
    "assert pt_out.shape == ms_out.shape\n",
    "loss = 1e-5                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out.detach().numpy(), ms_out.asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['language_mlp.gate_proj.weight', 'language_mlp.up_proj.weight', 'language_mlp.down_proj.weight', 'vision_mlp.gate_proj.weight', 'vision_mlp.up_proj.weight', 'vision_mlp.down_proj.weight']) odict_keys(['language_mlp.gate_proj.weight', 'language_mlp.up_proj.weight', 'language_mlp.down_proj.weight', 'vision_mlp.gate_proj.weight', 'vision_mlp.up_proj.weight', 'vision_mlp.down_proj.weight'])\n"
     ]
    }
   ],
   "source": [
    "#VisionExpertMLP Test\n",
    "from mindnlp.transformers import VisionExpertMLP\n",
    "\n",
    "ms_net = VisionExpertMLP(config_ms)\n",
    "pt_net = modeling_cogvlm_pt.VisionExpertMLP(config_ms)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "print(pt_dict.keys(),ms_net.parameters_dict().keys())\n",
    "\n",
    "assert pt_dict.keys() == ms_net.parameters_dict().keys()\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "length = 10\n",
    "input_np = np.random.random((batch,length,config_ms.hidden_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "\n",
    "input_id_np = np.random.randint(0,2,(batch,length))\n",
    "input_id_ms = mindspore.Tensor(input_id_np,dtype=mindspore.int64)\n",
    "input_id_pt = torch.Tensor(input_id_np)\n",
    "\n",
    "pt_out = pt_net(input_pt,input_id_pt)\n",
    "ms_out = ms_net(input_ms,input_id_ms)\n",
    "assert pt_out.shape == ms_out.shape\n",
    "loss = 1e-5                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out.detach().numpy(), ms_out.asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RotaryEmbedding Test\n",
    "from mindnlp.transformers import RotaryEmbedding\n",
    "\n",
    "ms_net = RotaryEmbedding(config_ms.hidden_size / config_ms.num_attention_heads)\n",
    "pt_net = modeling_cogvlm_pt.RotaryEmbedding(config_ms.hidden_size / config_ms.num_attention_heads)\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "#assert pt_dict.keys() == ms_net.parameters_dict().keys()\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "head = config_ms.num_attention_heads\n",
    "head_size = int(config_ms.hidden_size / config_ms.num_attention_heads)\n",
    "length = 10\n",
    "input_np = np.random.random((batch,head,length,head_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "\n",
    "pt_out = pt_net(input_pt,length)\n",
    "ms_out = ms_net(input_ms,length)\n",
    "assert pt_out[0].shape == ms_out[0].shape and pt_out[1].shape == ms_out[1].shape\n",
    "loss = 1e-5                                    # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out[1].detach().numpy(), ms_out[1].asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[128], dtype=Float32, value= [-3.47945690e-02,  1.08714193e-01,  3.95617485e-01, -1.52103141e-01, -3.21287751e-01,  2.00536698e-01, -1.87772989e-01,  3.79675329e-02, -1.01648659e-01,  3.41640621e-01, -2.43581384e-02, -1.53884172e-01, \n",
       "  3.79164040e-01,  5.20352364e-01,  3.06936830e-01,  8.66513774e-02,  7.20749944e-02,  7.73576945e-02,  7.07507193e-01, -2.76955590e-03,  8.57028961e-01,  8.27848911e-01,  8.58387172e-01,  8.75291228e-01, \n",
       "  6.39051795e-01,  8.25882077e-01,  7.21081734e-01,  2.97383517e-01,  7.37333179e-01,  6.06829345e-01,  2.18471959e-02,  7.84013271e-01,  8.68759334e-01,  5.10558367e-01,  4.83114511e-01,  4.20871407e-01, \n",
       "  7.13463843e-01,  2.14443922e-01,  7.69713223e-01,  2.88738549e-01,  2.58968383e-01,  8.08105886e-01,  1.73843682e-01,  8.62305462e-01,  9.77771699e-01,  3.18609297e-01,  9.26611960e-01,  5.65522194e-01, \n",
       "  2.58579522e-01,  4.52274054e-01,  7.99573421e-01,  6.66201174e-01,  4.94637638e-01,  2.56956816e-01,  7.25732625e-01,  3.13210666e-01,  3.30762297e-01,  7.46734381e-01,  5.58265984e-01,  1.85713828e-01, \n",
       "  6.87102973e-01,  2.57901907e-01,  7.60389864e-01,  4.78615224e-01,  1.04701877e+00,  1.05537963e+00,  5.69104016e-01,  7.01229274e-01,  8.22481513e-01,  1.11460948e+00,  6.70427501e-01,  1.05189502e+00, \n",
       "  5.96228898e-01,  3.60703975e-01,  6.73293054e-01,  8.96636307e-01,  4.32747543e-01,  3.48790437e-01,  1.01633704e+00,  7.81739950e-01,  5.72597869e-02,  5.70009291e-01,  2.88246840e-01,  2.87242115e-01, \n",
       "  1.35970563e-01,  2.71615267e-01,  5.41895926e-01,  1.10593423e-01,  9.10354078e-01,  7.98315585e-01,  3.98591429e-01,  1.36362776e-01,  1.00724339e+00,  5.72718978e-01,  4.23304230e-01,  2.07706496e-01, \n",
       "  2.11914644e-01,  8.00660372e-01,  5.23518741e-01,  9.86755788e-01,  9.57124889e-01,  2.07874537e-01,  1.00263011e+00,  9.87655878e-01,  8.00102532e-01,  3.93124253e-01,  2.24557683e-01,  1.04929693e-01, \n",
       "  5.37130646e-02,  8.21294010e-01,  9.46798146e-01,  7.73210943e-01,  7.59297848e-01,  4.15571094e-01,  6.41780794e-01,  6.71648324e-01,  2.57557184e-01,  7.38804758e-01,  1.46423027e-01,  2.93570578e-01, \n",
       "  5.89812994e-01,  5.39778709e-01,  7.04713881e-01,  5.82951903e-01,  4.96656448e-02,  9.77742732e-01,  7.63590336e-01,  8.80486429e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n",
    "    #return ops.cat((-x2, x1), dim=x1.ndim - 1)\n",
    "    return ops.cat((-x2, x1), axis=x1.ndim - 1)\n",
    "def apply_rotary_pos_emb_index_bhs(q, k, cos, sin, position_id,unsqueeze_dim=1):\n",
    "    # batch_size, num_head, seq_len, hidden_size\n",
    "    # cos, sin = F.embedding(position_id, cos.squeeze(1)).unsqueeze(1), \\\n",
    "    #     F.embedding(position_id, sin.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "    cos = cos.squeeze()\n",
    "    sin = sin.squeeze()\n",
    "    cos = cos[position_id].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_id].unsqueeze(unsqueeze_dim)\n",
    "    \n",
    "\n",
    "    q, k = (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q\n",
    "\n",
    "input_position_np = [np.arange(length,dtype=int) for i in range(batch)]\n",
    "input_position_ms = mindspore.Tensor(input_position_np,dtype=mindspore.int64)\n",
    "apply_rotary_pos_emb_index_bhs(input_ms,input_ms,ms_out[0],ms_out[1],input_position_ms)[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['rotary_emb.inv_freq', 'vision_expert_query_key_value.weight', 'vision_expert_dense.weight', 'language_expert_query_key_value.weight', 'language_expert_dense.weight']) odict_keys(['vision_expert_query_key_value.weight', 'vision_expert_dense.weight', 'language_expert_query_key_value.weight', 'language_expert_dense.weight'])\n"
     ]
    }
   ],
   "source": [
    "#VisionExpertAttention Test\n",
    "from mindnlp.transformers import VisionExpertAttention\n",
    "\n",
    "ms_net = VisionExpertAttention(config_ms)\n",
    "pt_net = modeling_cogvlm_pt.VisionExpertAttention(config_ms)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "print(pt_dict.keys(),ms_net.parameters_dict().keys())\n",
    "\n",
    "#assert pt_dict.keys() == ms_net.parameters_dict().keys()\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5923/2922467035.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  input_position_pt = torch.LongTensor(input_position_np)\n"
     ]
    }
   ],
   "source": [
    "batch = 2\n",
    "length = 64\n",
    "input_np = np.random.random((batch,length,config_ms.hidden_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "\n",
    "input_id_np = np.zeros((batch,length))\n",
    "input_id_np[:,int(length/2):] = 1\n",
    "input_id_ms = mindspore.Tensor(input_id_np,dtype=mindspore.int64)\n",
    "input_id_pt = torch.LongTensor(input_id_np)\n",
    "\n",
    "input_position_np = [np.arange(length,dtype=int) for i in range(batch)]\n",
    "input_position_ms = mindspore.Tensor(input_position_np,dtype=mindspore.int64)\n",
    "input_position_pt = torch.LongTensor(input_position_np)\n",
    "\n",
    "input_attn_mask_np = np.random.randint(0,2,(batch,1,length,length))\n",
    "input_attn_mask_ms = mindspore.Tensor(input_attn_mask_np,dtype=mindspore.int64)\n",
    "input_attn_mask_pt = torch.Tensor(input_attn_mask_np)\n",
    "\n",
    "pt_out = pt_net(input_pt,input_id_pt,input_position_pt,input_attn_mask_pt)\n",
    "ms_out = ms_net(input_ms,input_id_ms,input_position_ms,input_attn_mask_ms)\n",
    "assert pt_out[0].shape == ms_out[0].shape\n",
    "\n",
    "loss = 1e-5                                 # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out[0].detach().numpy(), ms_out[0].asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['self_attn.vision_expert_query_key_value.weight', 'self_attn.vision_expert_dense.weight', 'self_attn.language_expert_query_key_value.weight', 'self_attn.language_expert_dense.weight', 'mlp.language_mlp.gate_proj.weight', 'mlp.language_mlp.up_proj.weight', 'mlp.language_mlp.down_proj.weight', 'mlp.vision_mlp.gate_proj.weight', 'mlp.vision_mlp.up_proj.weight', 'mlp.vision_mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight']\n",
      "['self_attn.vision_expert_query_key_value.weight', 'self_attn.vision_expert_dense.weight', 'self_attn.language_expert_query_key_value.weight', 'self_attn.language_expert_dense.weight', 'mlp.language_mlp.gate_proj.weight', 'mlp.language_mlp.up_proj.weight', 'mlp.language_mlp.down_proj.weight', 'mlp.vision_mlp.gate_proj.weight', 'mlp.vision_mlp.up_proj.weight', 'mlp.vision_mlp.down_proj.weight', 'input_layernorm.weight', 'post_attention_layernorm.weight']\n"
     ]
    }
   ],
   "source": [
    "# cogvlmDecoderLayer\n",
    "from mindnlp.transformers import CogVLMDecoderLayer\n",
    "ms_net = CogVLMDecoderLayer(config_ms)\n",
    "pt_net = modeling_cogvlm_pt.CogVLMDecoderLayer(config_ms)\n",
    "\n",
    "pytorch_state_dict = pt_net.state_dict()\n",
    "\n",
    "pt_dict = pt_net.state_dict()\n",
    "\n",
    "pt_keys = list(pt_dict.keys())\n",
    "pt_keys.remove('self_attn.rotary_emb.inv_freq')\n",
    "ms_keys = list(ms_net.parameters_dict().keys())\n",
    "print(pt_keys)\n",
    "print(ms_keys)\n",
    "assert pt_keys == ms_keys\n",
    "for key, parameter in ms_net.parameters_and_names():\n",
    "    if 'inv' in key:\n",
    "        continue\n",
    "    parameter.set_data(mindspore.Tensor(pt_dict.get(key).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "length = 64\n",
    "input_np = np.random.random((batch,length,config_ms.hidden_size))\n",
    "input_ms = mindspore.Tensor(input_np,dtype=mindspore.float32)\n",
    "input_pt = torch.Tensor(input_np)\n",
    "\n",
    "input_id_np = np.zeros((batch,length))\n",
    "input_id_np[:,int(length/2):] = 1\n",
    "input_id_ms = mindspore.Tensor(input_id_np,dtype=mindspore.int64)\n",
    "input_id_pt = torch.LongTensor(input_id_np)\n",
    "\n",
    "input_position_np = [np.arange(length,dtype=int) for i in range(batch)]\n",
    "input_position_ms = mindspore.Tensor(input_position_np,dtype=mindspore.int64)\n",
    "input_position_pt = torch.LongTensor(input_position_np)\n",
    "\n",
    "input_attn_mask_np = np.random.randint(0,2,(batch,1,length,length))\n",
    "input_attn_mask_ms = mindspore.Tensor(input_attn_mask_np,dtype=mindspore.int64)\n",
    "input_attn_mask_pt = torch.Tensor(input_attn_mask_np)\n",
    "\n",
    "pt_out = pt_net(input_pt,input_id_pt,input_position_pt,input_attn_mask_pt)\n",
    "ms_out = ms_net(input_ms,input_id_ms,input_position_ms,input_attn_mask_ms)\n",
    "assert pt_out[0].shape == ms_out[0].shape\n",
    "\n",
    "loss = 1e-5                                 # 精度误差一般为1e-5，最大为1e-3，必须小于1e-3否则迁移出错\n",
    "assert np.allclose(pt_out[0].detach().numpy(), ms_out[0].asnumpy(), loss, loss) # 将结果全部转成array然后对比精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[64], dtype=Int64, value= [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, \n",
       " 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, \n",
       " 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_position_ids(x:\"mindspore.Tensor(B, L)\", attention_mask: Optional[\"mindspore.Tensor(B, L)\"] = None) -> \"mindspore.Tensor(B, L)\":\n",
    "    if attention_mask is not None:\n",
    "        tmp = x.copy()\n",
    "        tmp[~(attention_mask.bool())] = -1\n",
    "    else:\n",
    "        tmp = x.copy()\n",
    "    # image boi eoi token as LANGUAGE_TOKEN_TYPE\n",
    "    #is_boi_eoi = torch.zeros_like(x, dtype=torch.bool)\n",
    "    is_boi_eoi = ops.zeros_like(x, dtype=mindspore.bool_)\n",
    "    is_boi_eoi[:, 1:] |= (tmp[:, 1:] == VISION_TOKEN_TYPE) & (tmp[:, :-1] == LANGUAGE_TOKEN_TYPE)\n",
    "    is_boi_eoi[:, 0] |= (tmp[:, 0] == VISION_TOKEN_TYPE)\n",
    "    is_boi_eoi[:, :-1] |= (tmp[:, :-1] == VISION_TOKEN_TYPE) & (tmp[:, 1:] == LANGUAGE_TOKEN_TYPE)\n",
    "    is_boi_eoi[:, -1] |= (tmp[:, -1] == VISION_TOKEN_TYPE)\n",
    "    tmp[is_boi_eoi] = LANGUAGE_TOKEN_TYPE\n",
    "    # final position ids\n",
    "    # y = torch.zeros_like(x, dtype=mindspore.int64)\n",
    "    y = ops.zeros_like(x, dtype=mindspore.int64)\n",
    "    y[:, 1:] = (tmp[:, 1:] == LANGUAGE_TOKEN_TYPE) | ((tmp[:, 1:] == VISION_TOKEN_TYPE) & (tmp[:, :-1] == LANGUAGE_TOKEN_TYPE))\n",
    "    y = y.cumsum(axis=-1)\n",
    "    return y\n",
    "out = build_position_ids(input_id_ms)\n",
    "out[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('py39': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f688b49bf7bbd372001c59148eb4c8aaba45f80791d96530eef356c517b27051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
